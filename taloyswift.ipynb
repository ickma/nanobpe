{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/taylorswift.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanobpe.base import BaseTokenizer\n",
    "\n",
    "tokenizer = BaseTokenizer()\n",
    "\n",
    "tokenizer.train(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>ed Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>cent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>Caulfield, Keith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>No. 1 on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>erson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>Billboard 200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  token\n",
       "1119                el \n",
       "1117          ed Swift \n",
       "1114           New York\n",
       "1118                US \n",
       "1110               cent\n",
       "1103               open\n",
       "1084  Caulfield, Keith \n",
       "1079          No. 1 on \n",
       "1106             erson \n",
       "1121     Billboard 200 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = pd.DataFrame(tokenizer.ids_to_chars.values(), columns=['token'])\n",
    "\n",
    "vocab.tail(50).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Murphy, Sam (November 10, 2021). \"How 'Red' Became The Most Pivotal Record In Taylor Swift's Career\". Junkee. Retrieved November 10, 2021.\n",
      "( M)(ur)(ph)(y, )S(am)( (November )(10)(, 2021). \")(How )'(Re)d(' )B(ec)(ame )(The )(Mo)(st )P(iv)(ot)(al )(Record )(In )(Taylor Swift's )(Car)e(er)(\". )(Ju)nk(ee)(. Retrieved November )(10)(, 2021).\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def print_tokenized_line(ids,tokenizer):\n",
    "    chars =[]\n",
    "    for idx in ids:\n",
    "        if isinstance(tokenizer.vocab[idx], tuple):\n",
    "            chars.append(f'({tokenizer.ids_to_chars[idx]})')\n",
    "        else:\n",
    "            chars.append(tokenizer.ids_to_chars[idx])\n",
    "    return ''.join(chars)\n",
    "    \n",
    "\n",
    "for line in random.sample(text.split('\\n'), k=1):\n",
    "    print(line)\n",
    "    ids = tokenizer.encode(line)\n",
    "    print(print_tokenized_line(ids,tokenizer))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
